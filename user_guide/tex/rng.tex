\chapter{Random number generating}
\label{chap:Random number generating}

The library has a comprehensive \rng system to facilitate implementation of
Monte Carlo algorithms. A set of counter-based \rng{}s developed in
\textcite{Salmon:2011um} are re-implemented in the library with some
extensions. In addition wrappers for high performance \rng{}s in \mkl are also
provided, such that they can be used as \cppoo \rng engines. Some continuous
distributions, including all those in the standard library, are implemented.
Some of them are considerably faster than their standard library counter-parts.

Section~\ref{sec:Vectorized random number generating} introduces the library's
interface for vectorized random number generating. Section~\ref{sec:Notes on
  performance data} discusses the simple benchmark procedures later used in
this chapter. Section~\ref{sec:Counter-based RNG} details the counter-based
\rng{}s implemented in the library. Section~\ref{sec:Non-deterministic RNG}
briefly discusses the non-deterministic \rng{}s using \rdrand instructions and
section~\ref{sec:MKL RNG} shows how to use \rng{}s in the \mkl library as
\cppoo engines. Section~\ref{sec:Distributions} discusses the distributions
implemented by this library. Section~\ref{sec:Seeding} discusses how to seed
counter-based \rng{}s.

\section{Vectorized random number generating}
\label{sec:Vectorized random number generating}

Before we discuss other features, we first introduce a generic function
\verb|rng_rand|, which provides vectorized random number generating.
\begin{cppcode}
  template <typename RNGType>
  inline void rng_rand(
      RNGType &rng, std::size_t n, typename RNGType::result_type *r);

  template <typename RNGType, typename DistributionType>
  inline void rng_rand(RNGType &rng, const DistributionType &distribution,
      std::size_t n, typename DistributionType::result_type *r);
\end{cppcode}
The first is equivalent to generating random integers through a loop. For
example,
\begin{cppcode}
  rng_rand(rng, n, r.data());
  // is equivalent to
  for (std::size_t i = 0; i != n; ++i)
      r[i] = rng();
\end{cppcode}
The results of the two will always be exactly the same unless \rng{} is
non-deterministic. The second version of \verb|rng_rand| is also similar to the
loop,
\begin{cppcode}
  rng_rand(rng, distribution, n, r.data())
  // is similar to
  for (std::size_t i = 0; i != n; ++i)
      r[i] = distribution(rng);
\end{cppcode}
However, the results will not always be exactly the same, though the vectorized
version will generate random numbers with the same distribution as expected.

The advantage of using \verb|rng_rand| is that, if \verb|RNGType| or
\verb|DistributionType| are classes defined by this library, then vectorized
implementations might be used instead of using the loop. For some
distributions, the performance gain can be significant. Consider the following
dummy example,
\begin{cppcode}
  std::size_t n = 10000;
  RNG rng_vsmc;
  MKL_SFMT19937 rng_mkl;
  std::normal_distribution<double> rnorm_std;
  NormalDistribution<double> rnorm_vsmc;
  Vector<double> r(n);

  // Method 1
  for (std::size_t i = 0; i != n; ++i)
      r[i] = rnorm_std(rng_vsmc);

  // Method 2
  for (std::size_t i = 0; i != n; ++i)
      r[i] = rnorm_vsmc(rng_vsmc);

  // Method 3
  rng_rand(rng_vsmc, rnorm_vsmc, n, r.data());

  // Method 4
  rng_rand(rng_mkl, rnorm_vsmc, n, r.data());
\end{cppcode}
On the author's computer, on average method 1 costs 102 cycles to generate one
standard Normal random number. Method 2 costs 61 cycles while method 3 costs
only 14 cycles. The last, when the \rng{} engine is an \mkl{} \rng wrapper, the
\mkl library's routines are used and it costs only 10 cycles.

\section{Notes on performance data}
\label{sec:Notes on performance data}

Some of the following sections provide performance data. All performance data
in this chapter is measured in single core cycles per bytes (cpB) for \rng{}s,
or cycles per element for distributions (\verb|double| precision). The
processor used to measure the performance is an Intel Core i7-4960\textsc{hg}
\cpu. Three compilers are tested. The \llvm clang, the \gnu{} \gcc, and the
Intel \cpp compiler. The version information is shown below,
\begin{Verbatim}
  $ clang++ --version
  Apple LLVM version 7.3.0 (clang-703.0.29)
  Target: x86_64-apple-darwin15.4.0

  $ g++- --version
  g++-5 (Homebrew gcc 5.3.0) 5.3.0

  $ icpc --version
  icpc (ICC) 16.0.2 20160204
\end{Verbatim}
When multiple compilers are tested, they are labeled ``\llvm'', ``\gnu'', and
``Intel'', respectively in the tables. If only results from one compiler is
shown, then unless stated otherwise, it is the \llvm clang compiler. The
operating system is Mac OS~X~10.11.4.

For the performance of \rng{}s, we measure two situations. The first is
using the random integers one by one,
\begin{cppcode}
  RNG rng;
  UniformBitsDistribution<std::uint64_t> rbits;
  Vector<std::uint64_t> r(n);
  for (std:size_t i = 0; i != n; ++i)
      r[i] = rbits(rng);
\end{cppcode}
See section~\ref{sub:Uniform bits distribution} for details of
\verb|UniformBitsDistributions|. In short, it generates 64 random bits each
time \verb|rbits(rng)| is executed. The second is vectorized performance,
\begin{cppcode}
  rng_rand(rng, rbits, n, r.data());
\end{cppcode}
See section~\ref{sec:Vectorized random number generating} for details of
\verb|rng_rand|. In both cases, we repeat the experiments 100 times, each time
with the number of elements $n$ chosen randomly between 5,000 and 10,000. The
performance is measured in cpB. The two performance data are labeled ``Loop''
and ``\verb|rng_rand|'', respectively.

For the performance of distributions, we measure four situations. Take the
Normal distribution as an example, first, if the distribution is available in
the standard library, we measure the following case,
\begin{cppcode}
  RNG rng;
  std::normal_distribution<double> rnorm_std(0, 1);
  Vector<double> r(n);
  for (std::size_t i = 0; i = n; ++i)
      r[i] = rnorm_std(rng);
\end{cppcode}
Second, we measure the performance of the \vsmc library's implementation,
\begin{cppcode}
  NormalDistribution<double> rnorm_vsmc(0, 1);
  for (std::size_t i = 0; i = n; ++i)
      r[i] = rnorm_vsmc(rng);
\end{cppcode}
The third is the vectorized performance,
\begin{cppcode}
  rng_rand(rng, rnorm_vsmc, n, r.data());
\end{cppcode}
For all the three above, the \rng is \verb|ARSx8| (section~\ref{sub:AES-NI
  instructions based RNG}). The last measurement is the situation when \rng is
\verb|MKL_SFMT19937| (section~\ref{sec:MKL RNG}).
\begin{cppcode}
  MKL_SFMT19937 rng_mkl;
  rng_rand(rng_mkl, rnorm_vsmc, n, r.data());
\end{cppcode}
In this case, not only the \rng itself is faster, the distribution might also
use \mkl routines. The four performance data are labeled ``\std'', ``\vsmc'',
``\verb|rng_rand|'' and ``\mkl'', respectively. These measurements are in the
unit cycles per \verb|double| precision element.

Note that these performance data, especially those for \rng{}s, are for
reference only. The actual performance can differ considerably given different
compiler, operating system and hardware.

\section{Counter-based RNG}
\label{sec:Counter-based RNG}

The standard library provides a set of \rng engines (performance data in
table~\ref{tab:Performance of standard library RNG}). Unfortunately, none of
them are suitable for parallel computing without considerable efforts. The
development by \textcite{Salmon:2011um} made high performance parallel \rng
much more accessible.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|mt19937|       & 2.94 & 2.60 & 3.77 & 3.14 & 2.62 & 4.43 \\
    \verb|mt19937_64|    & 1.64 & 1.42 & 1.83 & 1.80 & 1.41 & 1.77 \\
    \verb|minstd_rand0|  & 5.11 & 5.41 & 8.02 & 5.10 & 5.40 & 6.94 \\
    \verb|minstd_rand|   & 3.92 & 4.69 & 6.48 & 3.91 & 5.32 & 5.82 \\
    \verb|ranlux24_base| & 6.83 & 6.55 & 7.73 & 6.77 & 6.48 & 7.49 \\
    \verb|ranlux48_base| & 4.13 & 3.91 & 4.92 & 4.20 & 3.87 & 5.15 \\
    \verb|ranlux24|      & 68.4 & 64.0 & 77.7 & 68.8 & 63.9 & 66.1 \\
    \verb|ranlux48|      & 156  & 141  & 181  & 158  & 139  & 155  \\
    \verb|knuth_b|       & 16.1 & 20.8 & 13.5 & 12.3 & 20.8 & 14.5 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of standard library RNG}
  \label{tab:Performance of standard library RNG}
\end{table}

The \rng{}s introduced in the paper use deterministic functions $f_k$, such
that, for a sequence $\{c_i = i\}_{i\ge0}$, the sequence $\{y_i =
f_k(c_i)\}_{i\ge0}$ appears as random. In addition, for $k_1 \ne k_2$,
$f_{k_1}$ and $f_{k_2}$ will generate two sequences that appear statistically
independent. Compared to more conventional \rng{}s which use recursions $y_i =
f_k(y_{i - 1})$, these counter-based \rng{}s are much easier to setup in a
parallelized environment. If $c$, the counter, is an unsigned integer with $b$
bits, and $k$, the key, is an unsigned integer with $d$ bits. Then for each
$k$, the \rng has a period $2^b$. And there can be at most $2^d$ independent
streams. Table~\ref{tab:Counter-based RNG} lists all counter-based \rng{}s
implemented in this library, along with the bits of the counter and the key.
They all conform to the \cppoo uniform \rng engine concept and output 32-bits
integers. For 64-bits integer output, a suffix \verb|_64| may be appended to
the corresponding \rng engine names. For example, \verb|Threefry4x64| and
\verb|Threefry4x64_64| generate the same 256-bits random integers internally.
The only difference is that \verb|operator()| of the former returns 32-bits
integers while the later returns 64-bits integers.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{lXX}
    \toprule
    Class & Counter bits & Key bits \\
    \midrule
    \verb|AES128x1|, \verb|ARS128x2|, \verb|AES128x4|, \verb|AES128x8|
    & $128$  & $128$  \\
    \verb|AES192x1|, \verb|ARS192x2|, \verb|AES192x4|, \verb|AES192x8|
    & $128$  & $192$  \\
    \verb|AES256x1|, \verb|ARS256x2|, \verb|AES256x4|, \verb|AES256x8|
    & $128$  & $256$  \\
    \verb|ARSx1|, \verb|ARS256x2|, \verb|ARSx4|, \verb|ARSx8|
    & $128$  & $128$  \\
    \verb|Philox2x32|    & $64$   & $32$   \\
    \verb|Philox2x64|    & $128$  & $64$   \\
    \verb|Philox4x32|    & $128$  & $64$   \\
    \verb|Philox4x64|    & $256$  & $128$  \\
    \verb|Threefry2x32|  & $64$   & $64$   \\
    \verb|Threefry2x64|  & $128$  & $128$  \\
    \verb|Threefry4x32|  & $128$  & $128$  \\
    \verb|Threefry4x64|  & $256$  & $256$  \\
    \verb|Threefry8x64|  & $512$  & $512$  \\
    \verb|Threefry16x64| & $1024$ & $1024$ \\
    \bottomrule
  \end{tabularx}
  \caption{Counter-based \rng; Each \rng engine may have suffix \texttt{\_64}}
  \label{tab:Counter-based RNG}
\end{table}

All \rng{}s in table~\ref{tab:Counter-based RNG} are actually type aliases.
More generally the library defines the following class template as the
interface,
\begin{cppcode}
  template <typename ResultType, typename Generator>
  class CounterEngine;
\end{cppcode}
where \verb|ResultType| shall be an unsigned integer type and \verb|Generator|
is the class that actually implement the algorithm. \verb|Generator| has at
least two member types, \verb|ctr_type| and \verb|key_type|, the types of the
counter and key. In addition, four methods are required,
\begin{cppcode}
  // The size of output in bytes
  static constexpr std::size_t size();

  // Reset the key of the generator
  void reset(const key_type &key);

  // Increment counter and generate a new random buffer
  template <typename ResultType>
  void operator()(ctr_type &ctr,
      std::array<ResultType, size() / sizeof(ResultType)> &buffer);

  // Increment counter and generate n new random buffers
  template <typename ResultType>
  void operator()(ctr_type &ctr, std::size_t n,
      std::array<ResultType, size() / sizeof(ResultType)> *buffer);
\end{cppcode}
The operators are not restricted to increment the counter only once for each
random buffer. The only restriction is that \verb|size()| is divisible by
\verb|sizeof(ResultType)|. This rule is enforced at compile-time. In the
remaining of this section, we introduce a few generators implemented by the
library. A few configuration macros of these \rng{}s are listed in
table~\ref{tab:Configuration macros for counter-based RNG}.

\begin{table}
  \begin{tabularx}{\textwidth}{XX}
    \toprule
    Macro & Default \\
    \midrule
    \verb|VSMC_RNG_AES128_ROUNDS|          & \verb|10| \\
    \verb|VSMC_RNG_AES192_ROUNDS|          & \verb|12| \\
    \verb|VSMC_RNG_AES256_ROUNDS|          & \verb|14| \\
    \verb|VSMC_RNG_ARS_ROUNDS|             & \verb|5|  \\
    \verb|VSMC_RNG_AES_NI_BLOCKS|          & \verb|8|  \\
    \verb|VSMC_RNG_PHILOX_ROUNDS|          & \verb|10| \\
    \verb|VSMC_RNG_PHILOX_VECTOR_LENGTH|   & \verb|4|  \\
    \verb|VSMC_RNG_THREEFRY_ROUNDS|        & \verb|20| \\
    \verb|VSMC_RNG_THREEFRY_VECTOR_LENGTH| & \verb|4|  \\
    \bottomrule
  \end{tabularx}
  \caption{Configuration macros for counter-based \rng{}}
  \label{tab:Configuration macros for counter-based RNG}
\end{table}

\subsection{\protect\aesni instructions based \protect\rng}
\label{sub:AES-NI instructions based RNG}

The \aesni instructions based \rng{}s in \textcite{Salmon:2011um} are
implemented in a more general form,
\begin{cppcode}
  template <typename KeySeqType, std::size_t Rounds, std::size_t Blocks>
  class AESNIGenerator;

  template <typename ResultType, typename KeySeqType, std::size_t Rounds,
      std::size_t Blocks>
  using AESNIEngine =
      CounterEngine<ResultType, AESNIGenerator<KeySeqType, Rounds, Blocks>>;
\end{cppcode}
where \verb|KeySeqType| is the class used to generate the sequence of round
keys; \verb|Rounds| is the number of rounds of \aes encryption to be performed.
See the reference manual for details of how to define the key sequence class.
The \aesni encryption instructions have a latency of seven or eight cycles,
while they can be issued at every cycle. Therefore better performance can be
achieved if multiple 128-bits random integers are generated at the same time.
This is specified by the template parameter \verb|Blocks|. Larger blocks, up to
eight, might improve performance but this is at the cost of larger state size.
Without going into details, there are four types of sequence of round keys
implemented by this library,
\begin{cppcode}
  template <std::size_t Rounds>
  using AES128KeySeq =
      internal::AESKeySeq<Rounds, internal::AES128KeySeqGenerator>;

  template <std::size_t Rounds>
  using AES192KeySeq =
      internal::AESKeySeq<Rounds, internal::AES192KeySeqGenerator>;

  template <std::size_t Rounds>
  using AES256KeySeq =
      internal::AESKeySeq<Rounds, internal::AES256KeySeqGenerator>;

  template <typename Constants = ARSConstants>
  using ARSKeySeq = internal::ARSKeySeqImpl<Constants>;
\end{cppcode}
and correspondingly four \rng engines,
\begin{cppcode}
  template <typename ResultType, std::size_t Rounds = VSMC_RNG_AES128_ROUNDS,
      std::size_t Blocks = VSMC_RNG_AES_NI_BLOCKS>
  using AES128Engine =
      AESNIEngine<ResultType, AES128KeySeq<Rounds>, Rounds, Blocks>;

  template <typename ResultType, std::size_t Rounds = VSMC_RNG_AES192_ROUNDS,
      std::size_t Blocks = VSMC_RNG_AES_NI_BLOCKS>
  using AES192Engine =
      AESNIEngine<ResultType, AES192KeySeq<Rounds>, Rounds, Blocks>;

  template <typename ResultType, std::size_t Rounds = VSMC_RNG_AES256_ROUNDS,
      std::size_t Blocks = VSMC_RNG_AES_NI_BLOCKS>
  using AES256Engine =
      AESNIEngine<ResultType, AES256KeySeq<Rounds>, Rounds, Blocks>;

  template <typename ResultType, std::size_t Rounds = VSMC_RNG_ARS_ROUNDS,
      std::size_t Blocks = VSMC_RNG_AES_NI_BLOCKS,
      typename Constants = ARSConstants>
  using ARSEngine =
      AESNIEngine<ResultType, ARSKeySeq<Constants>, Rounds, Blocks>;
\end{cppcode}
The first three are equivalent to \aes-128, \aes-192 and \aes-256 block ciphers
used in counter mode. The last is the \ars algorithm introduced by
\textcite{Salmon:2011um}. The last template parameter \verb|Constants| of
\verb|ARSKeySeq| and \verb|ARSEngine| is a trait class that defines the
constants of the Weyl's sequence. See \textcite{Salmon:2011um} for details. The
defaults are taken from the paper. To use an alternative pair of 64-bits
integers as the constants, one can define and use a trait class as the
following,
\begin{cppcode}
  template <std::size_t>
  struct NewWeylConstant;

  template<>
  struct NewWeylConstant<0>
  {
      static constexpr std::uint64_t value = FIRST_CONSTANT;
  };

  template<>
  struct NewWeylConstant<1>
  {
      static constexpr std::uint64_t value = SECOND_CONSTANT;
  };

  struct NewConstants
  {
      template <std::size_t I>
      using weyl = NewWeylConstant<I>;
  };

  using NewARS = ARSEngine<ResultType, Rounds, NewConstants>;
\end{cppcode}
Alternative methods are also possible. The only requirement is that, the
following expression,
\begin{cppcode}
  template <std::size_t I>
  using weyl = typename Constants::template weyl<I>;
\end{cppcode}
shall define a type that has a static constant expression member data
\verb|value| that is the \verb|I|\ith Weyl constant.

A few type aliases are defined for convenience. For example,
\begin{cppcode}
  using ARSx8    = ARSEngine<std::uint32_t, VSMC_RNG_ARS_ROUNDS, 8>;
  using ARSx8_64 = ARSEngine<std::uint64_t, VSMC_RNG_ARS_ROUNDS, 8>;
  using ARS      = ARSEngine<std::uint32_t>;
  using ARS_64   = ARSEngine<std::uint64_t>;
\end{cppcode}
The engine \verb|ARS| is the library's default \rng if \aesni instructions as
supported. Aliases for block sizes 1, 2, 4 and 8 are defined for all four
algorithms, as well as both 32- and 64-bits versions. These aliases are listed
in table~\ref{tab:Counter-based RNG}.

The performance of these engines depends on a few factors, such as \cpu types,
compilers, operating systems, etc. For example, the theoretical peak
performance for \verb|ARSx8| is 0.32 cpB on recent \cpu{}s. In realistic
situations, depending on the compiler, on the same computer values ranging from
0.35 to 1.2 cpB were observed by the author. In any case, such performance is
good enough even for the most demanding applications. The library does not
attempt to optimize the algorithm for any particular platform. In realistic
applications, the performance of \rng is unlikely to become a bottle neck. Note
that, the best performance is obtained with the vectorized \verb|rng_rand|
function (see section~\ref{sec:Vectorized random number generating}). The
performance data of all these \rng{}s are in table~\ref{tab:Performance of
  AES128Engine} to~\ref{tab:Performance of ARSEngine}.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|AES128x1|    & 1.09 & 5.54 & 4.59 & 0.79 & 4.19 & 4.09 \\
    \verb|AES128x2|    & 1.25 & 2.79 & 3.33 & 0.70 & 2.33 & 2.78 \\
    \verb|AES128x4|    & 0.87 & 1.72 & 1.84 & 0.67 & 1.30 & 1.45 \\
    \verb|AES128x8|    & 1.55 & 1.24 & 1.43 & 1.07 & 0.83 & 0.84 \\
    \verb|AES128x1_64| & 0.85 & 4.97 & 4.70 & 0.77 & 3.95 & 4.34 \\
    \verb|AES128x2_64| & 0.80 & 2.54 & 3.22 & 0.70 & 2.31 & 2.87 \\
    \verb|AES128x4_64| & 0.76 & 1.56 & 1.62 & 0.65 & 1.30 & 1.38 \\
    \verb|AES128x8_64| & 1.15 & 1.10 & 1.28 & 0.87 & 0.82 & 0.86 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \texttt{AES128Engine}}
  \label{tab:Performance of AES128Engine}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|AES192x1|    & 1.70 & 5.22 & 5.24 & 0.92 & 4.58 & 4.67 \\
    \verb|AES192x2|    & 1.38 & 3.06 & 3.25 & 0.82 & 2.63 & 2.78 \\
    \verb|AES192x4|    & 0.98 & 1.87 & 1.91 & 0.76 & 1.45 & 1.54 \\
    \verb|AES192x8|    & 1.32 & 1.32 & 1.44 & 0.93 & 0.91 & 0.93 \\
    \verb|AES192x1_64| & 1.07 & 5.03 & 5.75 & 0.89 & 4.55 & 5.06 \\
    \verb|AES192x2_64| & 0.94 & 2.83 & 3.13 & 0.83 & 2.61 & 2.80 \\
    \verb|AES192x4_64| & 0.88 & 1.69 & 1.82 & 0.76 & 1.44 & 1.56 \\
    \verb|AES192x8_64| & 1.26 & 1.19 & 2.04 & 1.00 & 0.91 & 1.21 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \texttt{AES192Engine}}
  \label{tab:Performance of AES192Engine}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|AES256x1|    & 1.96 & 5.80 & 5.96 & 1.05 & 5.16 & 5.35 \\
    \verb|AES256x2|    & 1.33 & 3.37 & 3.64 & 0.94 & 2.92 & 3.15 \\
    \verb|AES256x4|    & 1.12 & 2.01 & 2.06 & 0.89 & 1.62 & 1.71 \\
    \verb|AES256x8|    & 1.40 & 1.41 & 1.68 & 1.04 & 1.01 & 1.01 \\
    \verb|AES256x1_64| & 1.32 & 5.65 & 6.12 & 1.07 & 5.14 & 5.33 \\
    \verb|AES256x2_64| & 1.07 & 3.12 & 3.47 & 0.94 & 2.91 & 3.12 \\
    \verb|AES256x4_64| & 1.05 & 1.85 & 1.92 & 0.89 & 1.60 & 1.69 \\
    \verb|AES256x8_64| & 1.32 & 1.28 & 1.46 & 1.02 & 1.00 & 1.03 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \texttt{AES256Engine}}
  \label{tab:Performance of AES256Engine}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|ARSx1|    & 0.66 & 3.08 & 3.86 & 0.54 & 2.46 & 2.92 \\
    \verb|ARSx2|    & 0.92 & 2.13 & 2.69 & 0.43 & 1.61 & 1.98 \\
    \verb|ARSx4|    & 0.56 & 1.37 & 1.46 & 0.39 & 0.93 & 1.01 \\
    \verb|ARSx8|    & 0.66 & 1.05 & 1.07 & 0.34 & 0.63 & 0.69 \\
    \verb|ARSx1_64| & 0.55 & 2.97 & 3.18 & 0.54 & 2.46 & 2.51 \\
    \verb|ARSx2_64| & 0.46 & 1.99 & 2.24 & 0.43 & 1.60 & 1.76 \\
    \verb|ARSx4_64| & 0.48 & 1.27 & 1.62 & 0.40 & 0.92 & 1.20 \\
    \verb|ARSx8_64| & 0.49 & 0.94 & 1.04 & 0.34 & 0.61 & 0.75 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \texttt{ARSEngine}}
  \label{tab:Performance of ARSEngine}
\end{table}

\subsection{Philox}
\label{sub:Philox}

The Philox algorithm in \textcite{Salmon:2011um} is implemented in a more
general form,
\begin{cppcode}
  template <typename T, std::size_t K = VSMC_RNG_PHILOX_VECTOR_LENGTH,
      std::size_t Rounds = VSMC_RNG_PHILOX_ROUNDS,
      typename Constants = PhiloxConstants<T, K>>
  class PhiloxGenerator;

  template <typename ResultType, typename T = ResultType,
      std::size_t K = VSMC_RNG_PHILOX_VECTOR_LENGTH,
      std::size_t Rounds = VSMC_RNG_PHILOX_ROUNDS,
      typename Constants = PhiloxConstants<T, K>>
  using PhiloxEngine =
      CounterEngine<ResultType, PhiloxGenerator<T, K, Rounds, Constants>>;

  template <typename ResultType>
  using Philox2x32Engine = PhiloxEngine<ResultType, std::uint32_t, 2>;

  template <typename ResultType>
  using Philox4x32Engine = PhiloxEngine<ResultType, std::uint32_t, 4>;

  template <typename ResultType>
  using Philox2x64Engine = PhiloxEngine<ResultType, std::uint64_t, 2>;

  template <typename ResultType>
  using Philox4x64Engine = PhiloxEngine<ResultType, std::uint64_t, 4>;
\end{cppcode}
The default vector length and the number of rounds can be changed by
configuration macros listed in table~\ref{tab:Configuration macros for
  counter-based RNG}. Type aliases are also defined, as listed in
table~\ref{tab:Counter-based RNG}. The template parameter \verb|Counstants| is
similar to that of \verb|ARSEngine|. It defines the round constants of the
algorithm. The defaults are taken from the paper. See the reference manual for
details. The performance data is in table~\ref{tab:Performance of
  PhiloxEngine}.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|Philox2x32|    & 3.17 & 5.68 & 3.36 & 2.06 & 3.82 & 2.19 \\
    \verb|Philox4x32|    & 2.84 & 11.9 & 5.30 & 1.90 & 1.81 & 1.94 \\
    \verb|Philox2x64|    & 1.83 & 3.32 & 2.50 & 1.00 & 1.96 & 1.49 \\
    \verb|Philox4x64|    & 1.66 & 4.05 & 5.96 & 0.91 & 1.75 & 1.09 \\
    \verb|Philox2x32_64| & 2.83 & 5.52 & 2.91 & 2.06 & 3.82 & 2.13 \\
    \verb|Philox4x32_64| & 2.72 & 11.8 & 4.91 & 1.89 & 1.86 & 2.67 \\
    \verb|Philox2x64_64| & 1.63 & 3.11 & 3.02 & 0.99 & 1.97 & 2.20 \\
    \verb|Philox4x64_64| & 1.52 & 3.94 & 6.24 & 0.92 & 1.75 & 1.58 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \texttt{PhiloxEngine}}
  \label{tab:Performance of PhiloxEngine}
\end{table}

\subsection{Threefry}
\label{sub:Threefry}

The Threefry algorithm in \textcite{Salmon:2011um} is implemented in a more
general form,
\begin{cppcode}
  template <typename T, std::size_t K = VSMC_RNG_THREEFRY_VECTOR_LENGTH,
      std::size_t Rounds = VSMC_RNG_THREEFRY_ROUNDS,
      typename Constants = ThreefryConstants<T, K>>
  class ThreefryGenerator;

  template <typename ResultType, typename T = ResultType,
      std::size_t K = VSMC_RNG_THREEFRY_VECTOR_LENGTH,
      std::size_t Rounds = VSMC_RNG_THREEFRY_ROUNDS,
      typename Constants = ThreefryConstants<T, K>>
  using ThreefryEngine =
      CounterEngine<ResultType, ThreefryGenerator<T, K, Rounds, Constants>>;

  template <typename ResultType>
  using Threefry2x32Engine = ThreefryEngine<ResultType, std::uint32_t, 2>;

  template <typename ResultType>
  using Threefry4x32Engine = ThreefryEngine<ResultType, std::uint32_t, 4>;

  template <typename ResultType>
  using Threefry2x64Engine = ThreefryEngine<ResultType, std::uint64_t, 2>;

  template <typename ResultType>
  using Threefry4x64Engine = ThreefryEngine<ResultType, std::uint64_t, 4>;

  template <typename ResultType>
  using Threefry8x64Engine = ThreefryEngine<ResultType, std::uint64_t, 8>;

  template <typename ResultType>
  using Threefry16x64Engine = ThreefryEngine<ResultType, std::uint64_t, 16>;
\end{cppcode}
The default vector length and the number of rounds can be changed by
configuration macros listed in table~\ref{tab:Configuration macros for
  counter-based RNG}. Type aliases are also defined, as listed in
table~\ref{tab:Counter-based RNG}. The template parameter \verb|Counstants| is
similar to that of \verb|ARSEngine|. It defines the round constants of the
algorithm. The defaults are taken from the paper. See the reference manual for
details. The performance data is in table~\ref{tab:Performance of
  ThreefryEngine}.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|Threefry2x32|     & 5.60 & 5.45 & 20.3 & 4.64 & 4.87 & 3.90 \\
    \verb|Threefry4x32|     & 7.06 & 3.99 & 7.13 & 5.85 & 3.61 & 6.41 \\
    \verb|Threefry2x64|     & 5.59 & 3.15 & 3.18 & 4.67 & 2.56 & 2.23 \\
    \verb|Threefry4x64|     & 3.53 & 2.20 & 3.91 & 2.98 & 1.85 & 3.38 \\
    \verb|Threefry8x64|     & 3.79 & 1.80 & 2.73 & 2.38 & 1.46 & 2.29 \\
    \verb|Threefry16x64|    & 2.91 & 4.93 & 7.44 & 2.52 & 4.57 & 6.99 \\
    \verb|Threefry2x32_64|  & 4.69 & 5.15 & 19.9 & 4.66 & 4.87 & 3.84 \\
    \verb|Threefry4x32_64|  & 6.69 & 3.88 & 6.97 & 5.81 & 3.59 & 6.42 \\
    \verb|Threefry2x64_64|  & 5.01 & 3.04 & 2.70 & 4.65 & 2.54 & 2.22 \\
    \verb|Threefry4x64_64|  & 3.37 & 2.11 & 3.71 & 2.97 & 1.85 & 3.35 \\
    \verb|Threefry8x64_64|  & 3.06 & 1.66 & 2.65 & 2.41 & 1.47 & 2.29 \\
    \verb|Threefry16x64_64| & 2.83 & 4.84 & 7.16 & 2.53 & 4.55 & 6.93 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \texttt{ThreefryEngine}}
  \label{tab:Performance of ThreefryEngine}
\end{table}

\subsection{Default \protect\rng}
\label{sub:Default RNG}

Note that, not all \rng{}s defined by the library is available on all
platforms. The library also defines a type alias \verb|RNG| which is one of the
\rng{}s listed in table~\ref{tab:Counter-based RNG}. The preference is in the
order listed in table~\ref{tab:Default RNG}. The user can define the
configuration macro \verb|VSMC_RNG_TYPE| to override the choice made by the
library.

\begin{table}
  \begin{tabularx}{\textwidth}{XX}
    \toprule
    Class & Availability \\
    \midrule
    \verb|ARS|      & \verb|VSMC_HAS_AES_NI| \\
    \verb|Threefry| & Always available       \\
    \bottomrule
  \end{tabularx}
  \caption{Default RNG}
  \label{tab:Default RNG}
\end{table}

\section{Non-deterministic RNG}
\label{sec:Non-deterministic RNG}

If the \rdrand instructions are supported, the library also implements three
\rng{}s, \verb|RDRAND16|, \verb|RDRAND32| and \verb|RDRAND64|. They output 16-,
32-, and 64-bits random integers, respectively. The \rdrand instruction may not
return a random integer at all. The \rng engine will keep trying until it
succeeds. One can limit the maximum number of trials by defining the
configuration macro \verb|VSMC_RNG_RDRAND_NTRIAL_MAX|. A value of zero, the
default, means the number of trials is unlimited. If it is a positive number,
and if after the specified number of trials no random integer is return by the
\rdrand instruction, zero is returned.

\section{MKL RNG}
\label{sec:MKL RNG}

The \mkl library provides some high performance \rng{}s. The library implements
a wrapper class \verb|MKLEngine| that makes them accessible as \cppoo{}
generators. They are listed in table~\ref{tab:MKL RNG}. Note that, \mkl{}
\rng{}s performs best when they are used to generate vectors of random numbers.
These wrappers use a buffer to store such vectors. And thus they have much
larger state space than usual \rng{}s. Each \rng engines output by default
32-bits integers. Similar to the counter-based \rng{}s, 64-bits variants are
also defined. The performance data is in table~\ref{tab:Performance of MKL
  RNG}.

\begin{table}
  \begin{tabularx}{\textwidth}{XX}
    \toprule
    Class & \mkl \brng \\
    \midrule
    \verb|MKL_MCG59|         & \verb|VSL_BRNG_MCG59|         \\
    \verb|MKL_MT19937|       & \verb|VSL_BRNG_MT19937|       \\
    \verb|MKL_MT2203|        & \verb|VSL_BRNG_MT2203|        \\
    \verb|MKL_SFMT19937|     & \verb|VSL_BRNG_SFMT19937|     \\
    \verb|MKL_NONDETERM|     & \verb|VSL_BRNG_NONDETERM|     \\
    \verb|MKL_ARS5|          & \verb|VSL_BRNG_ARS5|          \\
    \verb|MKL_PHILOX4X32X10| & \verb|VSL_BRNG_PHILOX4X32X10| \\
    \bottomrule
  \end{tabularx}
  \caption{\mkl{} \rng; Each \rng engine may have suffix \texttt{\_64}}
  \label{tab:MKL RNG}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXXXX}
    \toprule
    & \multicolumn{3}{c}{Loop} & \multicolumn{3}{c}{\verb|rng_rand|} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    \rng  & \llvm & \gnu & Intel & \llvm & \gnu & Intel \\
    \midrule
    \verb|MKL_MCG59|           & 1.24 & 0.77 & 1.03 & 0.32 & 0.32 & 0.33 \\
    \verb|MKL_MCG59_64|        & 0.64 & 0.64 & 0.93 & 0.32 & 0.32 & 0.34 \\
    \verb|MKL_MT19937|         & 1.13 & 0.64 & 0.87 & 0.27 & 0.27 & 0.27 \\
    \verb|MKL_MT19937_64|      & 0.54 & 0.56 & 0.77 & 0.27 & 0.27 & 0.27 \\
    \verb|MKL_MT2203|          & 1.13 & 0.63 & 0.95 & 0.22 & 0.18 & 0.23 \\
    \verb|MKL_MT2203_64|       & 0.52 & 0.51 & 0.82 & 0.25 & 0.19 & 0.22 \\
    \verb|MKL_STMT19937|       & 1.12 & 0.63 & 0.92 & 0.16 & 0.16 & 0.18 \\
    \verb|MKL_STMT19937_64|    & 0.53 & 0.52 & 0.82 & 0.17 & 0.17 & 0.19 \\
    \verb|MKL_ARS5|            & 1.28 & 0.84 & 1.03 & 0.31 & 0.32 & 0.30 \\
    \verb|MKL_ARS5_64|         & 0.68 & 0.72 & 0.90 & 0.30 & 0.32 & 0.30 \\
    \verb|MKL_PHILOX4X32X10|   & 1.51 & 1.05 & 1.29 & 0.54 & 0.58 & 0.58 \\
    \verb|MKL_PHILOX4X32X10_64|& 0.90 & 0.95 & 1.19 & 0.54 & 0.59 & 0.58 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of \mkl{} \rng}
  \label{tab:Performance of MKL RNG}
\end{table}

\section{Multiple RNG streams}
\label{sec:Multiple RNG streams}

Earlier in section~\ref{sub:Particle} we introduced that \verb|particle.rng(i)|
returns an independent \rng instance. This is actually done through a class
template called \verb|RNGSet|. Three of them are implemented in the library.
They all have the same interface,
\begin{cppcode}
  RNGSet<RNG> rng_set(N); // A set of N RNGs
  rng_set.resize(n);      // Change the size of the set
  rng_set.seed();         // Seed each RNG in the set with Seed::instance()
  rng_set[i];             // Get a reference to the i-th RNG
\end{cppcode}
The first implementation is \verb|RNGSetScalar|. As its name suggests, it is
only a wrapper of a single \rng. All calls to \verb|rng_set[i]| returns a
reference to the same \rng. It is only useful when an \verb|RNGSet| interface
is required while the thread-safety and other issues are not important.

The second implementation is \verb|RNGSetVector|. It is an array of \rng{}s
with length $N$. It has memory cost $O(N)$. Many of the counter-based \rng{}s
have small state size and thus for moderate $N$, this cost is not an issue. The
method calls \verb|rng_set[i]| and \verb|rng_set[j]| return independent \rng{}s
if $i \ne j$.

Last, if \tbb is available, there is a third implementation \verb|RNGSetTBB|,
which uses thread-local storage (\tls). It has much smaller memory footprint
than \verb|RNGSetVector| while maintains better thread-safety. The performance
impact of using \tls is minimal unless the computation at the calling site is
trivial. For example,
\begin{cppcode}
  std::size_t eval_pre(SingleParticle<T> sp)
  {
      auto &rng = sp.rng();
      // using rng to initialize state
      // do some computation, likely far more costly than TLS
  }
\end{cppcode}
The type alias \verb|RNGSet| is defined to be \verb|RNGSetTBB| if \tbb is
available, otherwise defined to be \verb|RNGSetVector|. It is used by the
\verb|Particle| class template. One can replace the type of \rng set used by
\verb|Particle<T>| with a member type of \verb|T|. For example,
\begin{cppcode}
  class T
  {
      using rng_set_type = RNGSetScalar<RNG>;
  };
\end{cppcode}
will replace the type of the \rng set contained in \verb|Particle<T>|. Note
that, \verb|Particle<T>| itself does not use any \rng in the set.

\section{Distributions}
\label{sec:Distributions}

The library also provides implementations of some common distributions. They
all conforms to the \cppoo random number distribution concepts. Some of them
are the same as those in the \cppoo standard library, with \verb|CamelCase|
names. For example, \verb|NormalDistribuiton| can be used as a drop-in
replacement of \verb|std::normal_distribuiton|. This includes all of the
continuous distributions defined in the standard library.
Table~\ref{tab:Random number distributions} lists all the additional
distributions implemented. As stated in section~\ref{sec:Vectorized random
  number generating}, they support vectorized random number generating. In the
following sections we introduce each distributions included in the library.

\begin{table}
  \begin{tabularx}{\textwidth}{lX}
    \toprule
    Class & Notes \\
    \midrule
    \verb|LaplaceDistribution|
    & Parameters: location \verb|a|; scale \verb|b| \\
    \verb|LevyDistribution|
    & Parameters: location \verb|a|; scale \verb|b| \\
    \verb|ParetoDistribution|
    & Parameters: shape \verb|a|; scale \verb|b| \\
    \verb|RayleighDistribution|
    & Parameters: scale \verb|sigma| \\
    \bottomrule
  \end{tabularx}
  \caption{Random number distributions}
  \label{tab:Random number distributions}
\end{table}

\subsection{Uniform bits distribution}
\label{sub:Uniform bits distribution}

The class template,
\begin{cppcode}
  template <typename UIntType>
  class UniformBitsDistribution;
\end{cppcode}
is similar to the standard library's \verb|std::independent_bits_engine|,
except that it always generate full size random integers. That is, let $W$ be
the number of bits of \verb|UIntType|, then the output is uniform on the set
$S_W = \{0,\dots,2^W - 1\}$. For example,
\begin{cppcode}
  RNG rng;
  UniformBitsDistribution<std::uint32_t> dist;
  dist(rng); // Return 32-bits random integers
\end{cppcode}
Let $\rmin$ and $\rmax$ be the minimum and maximum of the output random
integers of the \rng. Let $R = \rmax - \rmin + 1$. Let $r_i$ be consecutive
output of \verb|rng()|. If there exists an integer $M > 0$ such that $R = 2^M$,
the output is,
\begin{equation*}
  U = \sum_{k = 0}^{K - 1} (r_k - \rmin) 2^{kM} \bmod 2^W
\end{equation*}
where $K = \Ceil{W / M}$. To see that it produces the desired results, note
that there is an one-to-one mapping between $(r_0,\dots,r_{K + 1})$ and
$\sum_{k=0}^{K-1}r_k^{kM}$, and $2^{KM}\equiv0\mod2^W$. Or a more intuitive way
is that, for $r_i - \rmin$ to be i.i.d.\ random integers on the set $S_M =
\{0,\dots,2^M - 1\}$, the \rng shall produce $M$ independent random bits. And
$U$ is formed by the lower $W$ bits of each $KM$ random bits produced by the
\rng. Unlike \verb|std::independent_bits_engine|, the calculation can be
vectorized, which leads to better performance. Note that, all constants in the
algorithm are computed at compile-time and the summation is fully unrolled, and
thus there is no runtime overhead. In the case $\rmin = 0$ and $M = W$, most
optimizing compilers shall be able to generate instructions such that the
distribution does exactly nothing and returns the results of \verb|rng()|
directly.

If there does not exist an integer $M > 0$ such that $R = 2^M$, then
\verb|std::indepdent_bits_engine| is used, and hence lower performance.

\subsection{Standard uniform distribution}
\label{sub:Standard uniform distribution}

All continuous distributions are built upon the standard uniform distribution.
And thus the performance and quality of the algorithm transferring random
integers to random floating point numbers on the set $[0, 1]$ are of critical
importance. The library provides five distributions for this purpose,
\begin{cppcode}
  template <typename RealType>
  class U01CCDistribution;

  template <typename RealType>
  class U01CODistribution;

  template <typename RealType>
  class U01OCDistribution;

  template <typename RealType>
  class U01OODistribution;

  template <typename RealType>
  class U01Distribution;
\end{cppcode}
The last \verb|U01Distribution| is used by all other distributions discussed
later. If the configuration macro \verb|VSMC_RNG_U01_USE_FIXED_POINT| is zero,
then it behaves similarly to \verb|std::generate_canonical| and thus
\verb|std::uniform_real_distribution|, otherwise it is the same as
\verb|U01CODistribution|. We now discuss the details of each distribution and
the implication of the configuration macros, which are listed in
table~\ref{tab:Configuraiton macros for standard uniform distributions}

\begin{table}
  \begin{tabularx}{\textwidth}{XX}
    \toprule
    Macro & Default \\
    \midrule
    \verb|VSMC_RNG_U01_USE_FIXED_POINT|   & \verb|1| \\
    \verb|VSMC_RNG_U01_USE_64BITS_DOUBLE| & \verb|0| \\
    \bottomrule
  \end{tabularx}
  \caption{Configuration macros for standard uniform distributions}
  \label{tab:Configuration macros for standard uniform distributions}
\end{table}

First of all, the random integers produced by \rng{}s are transferred to 32- or
64-bits intermediate random integers through \verb|UniformBitsDistribution|
before they are further converted to floating numbers. If \verb|RealType| is
\verb|long double| or the \rng{} produce random integers with at least 64 bits,
then 64-bits integers are used. If the configuration macro
\verb|VSMC_RNG_U01_USE_64BITS_DOUBLE| is defined to be an non-zero value and
\verb|RealType| is \verb|double|, then 64-bits integers are also used.
Otherwise, 32-bits random integers are used. Below, we let $W$ be the number of
bits of the random integers, and $M$ be the number of significant bits of
\verb|RealType|, which is usually 24 for \verb|float|, 53 for \verb|double|.
The situation for \verb|long double| is more complicated. On x86 and some other
platforms, it is 63. We also denote the input random integers as $U$ and the
output random real numbers as $Y$. Intermediate integer $V$ and real $X$ might
also be used.

\subsubsection{\texttt{U01CCDistribution}}

This distribution produce random real numbers on $[0, 1]$, with the lower and
upper bounds inclusive. The specific algorithm is as the following,
\begin{align*}
  P &= \min\{W - 1, M\} \\
  V &= \begin{cases}
    U &\text{if } P + 1 < W \\
    \Floor{(U \bmod 2^{W - 1}) / 2^{W - P -2}} &\text{otherwise}
  \end{cases} \\
  X &= (V \bmod 1) + V \\
  Y &= 2^{-(P + 1)} X
\end{align*}

\subsubsection{\texttt{U01CODistribution}}

This distribution produce random real numbers on $[0, 1)$, with the lower bound
inclusive and the upper bound never produced. The specific algorithm is as the
following,
\begin{align*}
  P &= \min\{W, M\} \\
  V &= \Floor{U / 2^{W - P}} \\
  X &= 2^{-P} V
\end{align*}

\subsubsection{\texttt{U01OCDistribution}}

This distribution produce random real numbers on $(0, 1]$, with the upper bound
inclusive and the lower bound never produced. The specific algorithm is as the
following,
\begin{align*}
  P &= \min\{W, M\} \\
  V &= \Floor{U / 2^{W - P}} \\
  X &= 2^{-P} V + 2^{-P}
\end{align*}

\subsubsection{\texttt{U01OODistribution}}

This distribution produce random real numbers on $(0, 1)$, with the lower and
upper bounds never produced. The specific algorithm is as the following,
\begin{align*}
  P &= \min\{W + 1, M\} \\
  V &= \Floor{U / 2^{W + 1 - P}} \\
  X &= 2^{-(P - 1)} V + 2^{-P}
\end{align*}

\subsubsection{\texttt{U01Distribution}}

It is now clear that the above four distributions actually produce ``fixed
point'' instead of ``floating point'' numbers. The output $X$ can be
represented exactly by the target \verb|RealType|. They have two advantages.
First, when it is important that the lower or upper bound is never produced, to
avoid underflow, overflow or other undefined behaviors in subsequent
calculations, they provide such assurance. Second, they usually can be executed
with only a couple of instructions by modern processors. And thus can have
better performance. Consider the following dummy example,
\begin{cppcode}
  std::size_t n = 10000;
  RNG rng;
  std::uniform_real_distribution<double> runif_std;
  U01CODistribution<double> runif_vsmc;
  Vector<double> r(n);

  // Method 1
  for (std::size_t i = 0; i != n; ++i)
      r[i] = runif_std(rng);

  // Method 2
  for (std::size_t i = 0; i != n; ++i)
      r[i] = runif_vsmc(rng);

  // Method 3
  rng_rand(rng, runif_vsmc, n, r.data());
\end{cppcode}
On the author's computer, on average method 1 costs 29 cycles to generate one
standard uniform random number. Method 2 costs 6.1 cycles while method 3 costs
only 2.4 cycles. The best performance is obtained when the output of \rng are
either 32 or 64 random bits. All \rng{}s in this library satisfy this
condition.

The main drawback is accuracy. If \verb|RealType| is \verb|float| or
\verb|long double|, then the difference is minimal, since the intermediate
random integers have more bits than the significant of the target floating
point type. In fact, if the \rng produces 32 random bits, then the results are
identical to the standard library for \verb|float|. If it produces 64 random
bits, then the results are also identical for \verb|double| or
\verb|long double|.

The situation is a bit more tricky in the case of \verb|double| and the
intermediate random integers are 32-bits. In this case,
\verb|U01CODistribution| can only produce $2^{32}$ distinctive values while
\verb|double| can represent much more values exactly within the range $[0, 1)$.
In contrast, the standard library will use at least 53 random bits. This will
not matter in most realistic applications. In fact, random numbers produced by
\verb|U01CODistribution| passes all tests in the
{\lnfigures\tbfigures TestU01}%
\footnote{\url{http://www.iro.umontreal.ca/~simardr/testu01/tu01.html}}
library that \verb|std::uniform_real_distribution| would pass, for a good \rng.
In other words, the quality of the \rng is the dominating factor.

However, there are situations where one do want the extra precision. For
example, the library implement the Normal distribution using the standard
Box-Muller method \parencite{Box:1958hv}, for performance consideration. Better
accuracy at the tail can only be archived by using procedures that can produce
values closer to zero than $2^{-32}$. In this case, there are two solutions.
The first is to define the configuration macro
\verb|VSMC_RNG_U01_USE_FIXED_POINT| to zero, and thus \verb|U01Distribution| is
no longer the same as \verb|U01CODistribution|. Instead, it behaves similarly
to \verb|std::generate_canonical|. More specifically,
\begin{align*}
  P &= \Floor{(W + M - 1) / W} \\
  K &= \max\{1, P\} \\
  X &= \sum_{k=0}^{K - 1} U_k 2^{-(K - k)W}
\end{align*}
The other solution is to define the configuration macro
\verb|VSMC_RNG_U01_USE_64BITS_DOUBLE| to a non-zero value, such that the
intermediate random integers will always be 64-bits for \verb|double| output.
This configuration macro also affects all other four distributions discussed
earlier.

\subsection{Distributions using the inverse method}
\label{sub:Distributions using the inverse method}

The following class templates implement distributions using the inverse method.
Their distribution functions are shown in the comments on each class template.
\begin{cppcode*}{texcomments}
  // $F(x) = \frac{1}{\pi}\arctan\Round[Big]{\frac{x - a}{b}} + \frac{1}{2}$
  template <typename RealType>
  class CauchyDistribution;

  // $F(x) = 1 - \EE^{-\lambda x}$
  template <typename RealType>
  class ExponentialDistribution;

  // $F(x) = \exp\Curly[Big]{-\exp\Round[Big]{-\frac{x - a}{b}}}$
  template <typename RealType>
  class ExtremeValueDistribution;

  // $F(x) = \frac{1}{2} + \frac{1}{2}\mathrm{sgn}(x - a)\Round[Big]{1 - \exp\Curly[Big]{-\frac{\Abs{x - a}}{b}}}$
  template <typename RealType>
  class LaplaceDistribution;

  // $F(x) = \Round[Big]{1 + \exp\Curly[Big]{-\frac{x - a}{b}}}^{-1}$
  template <typename RealType>
  class LogisticDistribution;

  // $F(x) = 1 - \Round[Big]{\frac{b}{x}}^a$
  template <typename RealType>
  class ParetoDistribution;

  // $F(x) = 1 - \exp\Curly[Big]{-\frac{x^2}{2\sigma^2}}$
  template <typename RealType>
  class RayleighDistribution;

  // $F(x) = \frac{x - a}{b - a}$
  template <typename RealType>
  class UniformRealDistribution;

  // $F(x) = 1 - \exp\Curly[Big]{-\Round[Big]{\frac{x}{b}}^a}$
  template <typename RealType>
  class WeibullDistribution;
\end{cppcode*}
The performance data of these distributions is in table~\ref{tab:Performance of
  distributions using the inverse method}.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|Cauchy(0,1)|           & 100  & 72.7 & 14.4 & 10.5 \\
    \verb|Exponential(1)|        & 70.1 & 58.5 & 7.72 & 5.98 \\
    \verb|ExtremeValue(0,1)|     & 119  & 66.4 & 12.4 & 11.0 \\
    \verb|Laplace(0,1)|          & --   & 75.5 & 9.38 & 9.37 \\
    \verb|Logistic(0,1)|         & --   & 38.7 & 16.6 & 12.6 \\
    \verb|Pareto(1,1)|           & --   & 95.1 & 17.4 & 11.0 \\
    \verb|Rayleigh(1)|           & --   & 73.5 & 11.5 & 9.21 \\
    \verb|UniformReal(-0.5,0.5)| & 30.1 & 10.9 & 2.65 & 1.03 \\
    \verb|Weibull(1,1)|          & 144  & 27.8 & 7.58 & 6.18 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of distributions using the inverse method}
  \label{tab:Performance of distributions using the inverse method}
\end{table}

\subsection{Normal and related distribution}
\label{sub:Normal and related distribuiton}

The class template
\begin{cppcode*}{texcomments}
  // $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Curly[Big]{-\frac{(x-\mu)^2}{2\sigma^2}}$
  template <typename RealType>
  class NormalDistribution;
\end{cppcode*}
implements the Normal distribution with the Box-Muller method
\parencite{Box:1958hv}. It is also used to implement the Log-Normal
and Levy distributions. Below in the comments, $z$ is a the standard Normal
random number and $x$ is the target distribution random number,
\begin{cppcode*}{texcomments}
  // $x = \EE^{m + s z}$
  template <typename RealType>
  class LognormalDistribution;

  // $x = a + \frac{b}{z^2}$
  template <typename RealType>
  class LevyDistribution;
\end{cppcode*}
The performance data of these distributions is in table~\ref{tab:Performance of
  Normal and related distributions}.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|Normal(0,1)|    & 102  & 61.6 & 14.3 & 9.59 \\
    \verb|Lognormal(0,1)| & 124  & 79.8 & 18.4 & 15.2 \\
    \verb|Levy(0,1)|      & --   & 68.1 & 21.5 & 16.3 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of Normal and related distributions}
  \label{tab:Performance of Normal and related distributions}
\end{table}

The library also implements the multivariate Normal distribution.
\begin{cppcode}
  template <typename RealType, std::size_t Dim>
  class NormalMVDistribution;
\end{cppcode}
If \verb|Dim| is zero (\verb|Dynamic|), then the distribution can be
constructed with,
\begin{cppcode}
  explicit NormalMVDistribution(std::size_t dim,
      const result_type *mean = nullptr, const result_type *chol = nullptr);
\end{cppcode}
Otherwise, if \verb|Dim| is positive integer, it can be constructed with static
size,
\begin{cppcode}
  explicit NormalMVDistribution(
      const result_type *mean = nullptr, const result_type *chol = nullptr);
\end{cppcode}
In either case, the parameter \verb|mean| is the mean vector. If it is a null
pointer, then it is assumed it is a zero vector. The parameter \verb|chol| is a
$d(d + 1)/2$-vector, where $d$ is dimension. The vector is the lower triangular
elements of the Cholesky decomposition of the covariance matrix, packed row by
row. Libraries such as \lapack has routines to generate such a matrix.
Alternatively, one can use the covariance functionalities in the library. See
section~\ref{sec:Sample covariance}, which also provides a concrete example of
using the multivariate Normal distribution.

\subsection{Gamma and related distribution}
\label{sub:Gamma and related distribution}

The class template
\begin{cppcode*}{texcomments}
  // $f(x) = \frac{\EE^{-x/\beta}}{\Gamma(\alpha)}\beta^{-\alpha}x^{\alpha-1}$
  template <typename RealType>
  class GammaDistribution;
\end{cppcode*}
implements the Gamma distribution. The specific algorithms depends on the
parameters. If $\alpha = 1$, it becomes the exponential distribution.
Otherwise, if $\alpha < 0.6$, it is generated through transformation of
exponential power distribution \parencite[sec~2.6]{Devroye:1986gi}. If
$0.6\le\alpha<1$, then rejection method from the Weibull distribution is used
\parencite[sec.~3.4]{Devroye:1986gi}. If $\alpha > 1$, then the method in
\textcite{Marsaglia:2000vq} is used. There are three related distributions,
\begin{cppcode}
  // GammaDistribution<RealType> rgamma(n / 2, 2);
  // x = rgamma(rng);
  template <typename RealType>
  class ChiSquaredDistribution;

  // ChiSquaredDistribution<RealType> rchi1(m);
  // ChiSquaredDistribution<RealType> rchi2(n);
  // x = (rchi1(rng) / m) / (rchi2(rng) / n);
  template <typename RealType>
  class FisherFDistribution;

  // NormalDistribution<RealType> rnorm(0, 1);
  // ChiSquaredDistribution<RealType> rchi(n);
  // x = rnorm(rng) * std::sqrt(n / rchi(rng()));
  template <typename RealType>
  class StudentTDistribution;
\end{cppcode}
They implement the $\chi^2$-distribution, the Fisher $F$-distribution, and the
Student's $t$-distribution, respectively. The performance data of these
distributions, for different parameters is in table~\ref{tab:Performance of
  Gamma distribution} to~\ref{tab:Performance of Student's t-distribution}.

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|Gamma(1,1)|   & 78.4 & 33.9 & 7.56 & 6.45 \\
    \verb|Gamma(0.1,1)| & 175  & 112  & 30.4 & 32.5 \\
    \verb|Gamma(0.5,1)| & 217  & 150  & 54.3 & 46.3 \\
    \verb|Gamma(0.7,1)| & 224  & 165  & 42.9 & 37.2 \\
    \verb|Gamma(0.9,1)| & 219  & 137  & 33.1 & 27.9 \\
    \verb|Gamma(1.5,1)| & 273  & 130  & 32.0 & 27.0 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of Gamma distribution}
  \label{tab:Performance of Gamma distribution}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|ChiSquared(0.2)| & 180  & 134  & 31.2 & 32.9 \\
    \verb|ChiSquared(1)|   & 218  & 164  & 54.3 & 46.4 \\
    \verb|ChiSquared(1.5)| & 232  & 162  & 40.2 & 34.3 \\
    \verb|ChiSquared(2)|   & 101  & 38.3 & 7.55 & 6.44 \\
    \verb|ChiSquared(3)|   & 275  & 141  & 34.3 & 27.4 \\
    \verb|ChiSquared(30)|  & 240  & 138  & 29.9 & 24.6 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of $\chi^2$ distribution}
  \label{tab:Performance of chi-squared distribution}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|FisherF(0.2,0.2)| & 349  & 277  & 77.6 & 82.1 \\
    \verb|FisherF(0.2,1)|   & 388  & 344  & 114  & 98.6 \\
    \verb|FisherF(0.2,1.5)| & 404  & 354  & 105  & 86.3 \\
    \verb|FisherF(0.2,2)|   & 266  & 183  & 48.3 & 51.5 \\
    \verb|FisherF(0.2,3)|   & 459  & 319  & 77.9 & 75.7 \\
    \verb|FisherF(1,0.2)|   & 388  & 311  & 107  & 97.9 \\
    \verb|FisherF(1,1)|     & 427  & 337  & 134  & 111  \\
    \verb|FisherF(1,1.5)|   & 488  & 345  & 130  & 99.2 \\
    \verb|FisherF(1,2)|     & 308  & 222  & 78.3 & 66.4 \\
    \verb|FisherF(1,3)|     & 472  & 379  & 108  & 92.0 \\
    \verb|FisherF(2,0.2)|   & 254  & 188  & 48.4 & 50.4 \\
    \verb|FisherF(2,1)|     & 298  & 221  & 78.6 & 66.5 \\
    \verb|FisherF(2,1.5)|   & 304  & 236  & 71.9 & 50.4 \\
    \verb|FisherF(2,2)|     & 159  & 104  & 21.1 & 19.8 \\
    \verb|FisherF(2,3)|     & 344  & 209  & 48.3 & 43.8 \\
    \verb|FisherF(3,0.2)|   & 433  & 299  & 77.4 & 75.3 \\
    \verb|FisherF(3,1)|     & 473  & 325  & 107  & 91.4 \\
    \verb|FisherF(3,1.5)|   & 503  & 377  & 107  & 74.9 \\
    \verb|FisherF(3,2)|     & 350  & 199  & 47.9 & 42.4 \\
    \verb|FisherF(3,3)|     & 528  & 358  & 77.7 & 67.2 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of Fisher's $F$-distribution}
  \label{tab:Performance of Fisher's F-distribution}
\end{table}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{2in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|StudentT(0.2)| & 309  & 209  & 61.4 & 59.6 \\
    \verb|StudentT(1)|   & 338  & 260  & 93.3 & 73.9 \\
    \verb|StudentT(1.5)| & 377  & 249  & 102  & 59.5 \\
    \verb|StudentT(2)|   & 187  & 117  & 32.1 & 26.7 \\
    \verb|StudentT(3)|   & 372  & 234  & 59.6 & 59.7 \\
    \verb|StudentT(30)|  & 333  & 232  & 57.2 & 57.2 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of Student's $t$-distribution}
  \label{tab:Performance of Student's t-distribution}
\end{table}

\subsection{Beta distribution}
\label{sub:Beta distribution}

The class template
\begin{cppcode*}{texcomments}
  // $f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1 - x)^{\beta - 1}$
  template <typename RealType>
  class BetaDistribution;
\end{cppcode*}
implements the Beta distribution. The specific algorithm used depend on the
parameters. If $\alpha = 1/2$ and $\beta = 1/2$, or $\alpha = 1$ or $\beta =
1$, then the inverse method is used. If $\alpha > 1$ and $\beta > 1$, the
method in \textcite{Cheng:1978jl} is used. Otherwise, let $K = 0.852$, $C =
-0.956$, and $D = \beta + K\alpha^2 + C$. If $\alpha < 1$, $\beta < 1$ and $D
\le 0$, then Jöhnk's method \parencite[sec.~3.5]{Devroye:1986gi} is used. In
all other cases, one of the switching algorithms in \textcite{Atkinson:1979es}
is used. Note that, there is no vectorized implementation at the moment for the
switching algorithms. In other cases, the vectorized generating shall provide
considerable speedup. The performance data is in table~\ref{tab:Performance of
  Beta distribution}

\begin{table}
  \tbfigures
  \begin{tabularx}{\textwidth}{p{1in}XXXX}
    \toprule
    Distribution & \std & \vsmc & \verb|rng_rand| & \mkl \\
    \midrule
    \verb|Beta(0.5,0.5)| & 431  & 42.9 & 10.3 & 37.4 \\
    \verb|Beta(1,1)|     & 144  & 16.2 & 2.48 & 8.73 \\
    \verb|Beta(1,0.5)|   & 288  & 65.1 & 12.2 & 8.63 \\
    \verb|Beta(1,1.5)|   & 406  & 65.0 & 11.8 & 8.61 \\
    \verb|Beta(0.5,1)|   & 288  & 62.4 & 11.7 & 8.61 \\
    \verb|Beta(1.5,1)|   & 388  & 62.4 & 11.6 & 8.59 \\
    \verb|Beta(1.5,1.5)| & 632  & 174  & 45.9 & 41.2 \\
    \verb|Beta(0.3,0.3)| & 418  & 148  & 54.0 & 35.4 \\
    \verb|Beta(0.9,0.9)| & 469  & 172  & 172  & 47.8 \\
    \verb|Beta(1.5,0.5)| & 554  & 203  & 202  & 73.3 \\
    \verb|Beta(0.5,1.5)| & 534  & 203  & 202  & 57.4 \\
    \bottomrule
  \end{tabularx}
  \caption{Performance of Beta distribution}
  \label{tab:Performance of Beta distribution}
\end{table}

\section{Seeding}
\label{sec:Seeding}

The singleton class template \verb|SeedGenerator| can be used to generate
distinctive seed sequentially. For example,
\begin{cppcode}
  auto &seed = SeedGenerator<void, unsigned>::instance();
  RNG rng1(seed.get()); // Construct rng1
  RNG rng2(seed.get()); // Construct rng2 with another seed
\end{cppcode}
The first argument to the template can be any type. For different types,
different instances of \verb|SeedGenerator| will be created. Thus, the seeds
generated by \verb|SeedGenerator<T1>| and \verb|SeedGenerator<T2>| will be
independent. The second parameter is the type of the seed values. It can be an
unsigned integer type. Classes such as \verb|Particle<T>| will use the
generator of the following type,
\begin{cppcode}
  using Seed = SeedGenerator<NullType, VSMC_SEED_RESULT_TYPE>;
\end{cppcode}
where \verb|VSMC_SEED_RESULT_TYPE| is a configuration macro which is defined to
\verb|unsigned| by default.

One can save and set the seed generator using standard \cpp streams. For
example
\begin{cppcode}
  std::ifstream seed_txt("seed.txt");
  if (seed_txt.good())
      seed_txt >> Seed::instance(); // Read seed from a file
  else
      Seed::instance().set(101);    // The default seed
  seed_txt.close();
  // The program
  std::ofstream seed_txt("seed.txt");
  seed_txt << Seed::instance();     // Write the seed to a file
  seed_txt.close();
\end{cppcode}
This way, if the simulation program need to be repeated multiple times, each
time is will use a different set of seeds.

A single seed generator is enough for a single computer program. However, it is
more difficult to ensure that each computing node has a distinctive set of
seeds in a distributed system. A simple solution is to use the \verb|modulo|
method of \verb|SeedGenerator|. For example,
\begin{cppcode}
  Seed::instance().modulo(n, r);
\end{cppcode}
where $n$ is the number of processes and $r$ is the rank of the current node.
After this call, all seeds generated will belong to the equivalent class $s
\equiv r \mod n$. Therefore, no two nodes will ever generate the same seeds.
